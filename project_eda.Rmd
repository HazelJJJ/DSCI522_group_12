---
title: "project_eda"
author: "Lara Habashy"
date: "20/11/2020"
output: html_document
---
---
title: "Exploratory data analysis of the Wisconsin Breast Cancer data set"
output: github_document
bibliography: ../doc/breast_cancer_refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
library(knitr)
library(caret)
library(readxl)
set.seed(2020)
library(pacman)
p_load(
  tidyverse, skimr, feather, magrittr, lubridate,
  microbenchmark, tictoc, furrr,
  tidytext,
  xts, zoo, imputeTS,
  scales, dygraphs, plotly, htmlwidgets, viridis, ggrepel, gridExtra, ggthemes,
  tsibble
)
```

# Load the data
```{r}
dat <- read_excel("default of credit card clients.xls")
dat <- janitor::row_to_names(dat,1)
dat <- dat %>% janitor::clean_names()
```


# Partition the data set into training and test sets

Before splitting the data set into training (75%) and testing (25%) sets, we inspect class balance to detect any imbalance in the target class which we attempt to correct. We also drop the `ID` features as it's irrelavant.

```{r split data}
# drop id and convert class to factor
cred_data <- dat %>% 
    select(-id) %>% 
    mutate(class = as.factor(default_payment_next_month))

target <- cred_data$default_payment_next_month
prop.table(table(target)) 
  
# split into training and test data sets
training_rows <- cred_data %>% 
    select(default_payment_next_month) %>% 
    pull() %>%
    createDataPartition(p = 0.75, list = FALSE)

training_data <- cred_data %>% slice(training_rows)
test_data <- cred_data %>% slice(-training_rows)



train_counts <- training_data$default_payment_next_month
prop.table(table(train_counts)) 

test_counts <- test_data$default_payment_next_month
prop.table(table(test_counts)) 
```


# Exploratory analysis on the training data set

> Tiff's words "To look at whether each of the predictors might be useful to predict the tumour class, we plotted the distributions of each predictor from the training data set and coloured the distribution by class (benign: blue and malignant: orange). In doing this we see that class distributions for all of the mean and max predictors for all the measurements overlap somewhat, but do show quite a difference in their centres and spreads. This is less so for the standard error (se) predictors. In particular, the standard errors of fractal dimension, smoothness, symmetry and texture look very similar in both the distribution centre and spread. Thus, we might choose to omit these from our model."

```{r predictor distributions, fig.width=8, fig.height=10, fig.cap="Distribution of training set predictors for the benign (B) and malignant (M) tumour cases."}
training_data %>% 
  gather(key = predictor, value = value, -class) %>% 
  mutate(predictor = str_replace_all(predictor, "_", " ")) %>% 
  ggplot(aes(x = value, y = class, colour = class, fill = class)) +
      facet_wrap(. ~ predictor, scale = "free", ncol = 4) +
      geom_density_ridges(alpha = 0.8) +
      scale_fill_tableau() +
      scale_colour_tableau() +
      guides(fill = FALSE, color = FALSE) +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
```

